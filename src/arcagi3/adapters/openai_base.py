import abc
import logging
import time
from time import sleep
from typing import Any, Dict, List, Optional

from dotenv import load_dotenv
from openai.types import CompletionUsage
from openai.types.chat import ChatCompletion, ChatCompletionMessage
from openai.types.chat.chat_completion import Choice as OpenAIChoice

from arcagi3.schemas import (APIType, Attempt, CompletionTokensDetails, Cost,
                             StreamResponse, Usage)
from arcagi3.utils.retry import retry_with_exponential_backoff

from .provider import ProviderAdapter

load_dotenv()

logger = logging.getLogger(__name__)


# Helper classes for responses API mock structure
class _ResponsesContent:
    def __init__(self, text):
        self.text = text
        self.type = "output_text"

class _ResponsesOutput:
    def __init__(self, text):
        self.content = [_ResponsesContent(text)]
        self.role = "assistant"
        self.type = "message"

class _ResponsesResponse:
    def __init__(self, model_name, content, usage_data, response_id, finish_reason="stop"):
        self.id = response_id or "stream-response"
        self.model = model_name
        self.object = "response"
        self.output = [_ResponsesOutput(content)]
        self.finish_reason = finish_reason
        self.usage = usage_data


class OpenAIBaseAdapter(ProviderAdapter, abc.ABC):
    
    def __init__(self, config: str):
        super().__init__(config)
        self._last_consumed_stream = None  # Cache for stream consumption

    def _filter_api_kwargs(self, kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """
        Filter out internal configuration parameters that should not be passed to the API.
        These are application-level settings, not API parameters.
        Uses INTERNAL_API_PARAMS from ProviderAdapter base class.
        """
        return {k: v for k, v in kwargs.items() if k not in ProviderAdapter.INTERNAL_API_PARAMS}

    @abc.abstractmethod
    def make_prediction(self, prompt: str, task_id: Optional[str] = None, test_id: Optional[str] = None, pair_index: int = None) -> Attempt:
        """
        Make a prediction with the model and return an Attempt object.
        Subclasses must implement this to handle provider-specific response parsing.
        """
        pass

    def _call_ai_model(self, prompt: str) -> Any:
        """
        Call the appropriate OpenAI API based on the api_type
        """
        
        # Validate that background and stream are not both enabled for responses API
        stream_enabled = self.model_config.kwargs.get('stream', False) or getattr(self.model_config, 'stream', False)
        messages = [{"role": "user", "content": prompt}]
        if self.model_config.api_type == APIType.CHAT_COMPLETIONS:
            if stream_enabled:
                return self._chat_completion_stream(messages)
            return self._chat_completion(messages)
        
        else:  # APIType.RESPONSES
            # account for different parameter names between chat completions and responses APIs
            self._normalize_to_responses_kwargs()
            background_enabled = getattr(self.model_config, 'background', False)
            if stream_enabled and background_enabled:
                raise ValueError("Cannot enable both streaming and background for the responses API type.")
            if stream_enabled:
                return self._responses_stream(messages)
            return self._responses(messages)
    
    def _chat_completion(self, messages: List[Dict[str, str]]) -> Any:
        """
        Make a call to the OpenAI Chat Completions API
        """
        api_kwargs = self._filter_api_kwargs(self.model_config.kwargs)
        logger.debug(f"Calling OpenAI API with model: {self.model_config.model_name} and kwargs: {api_kwargs}")
    
        
        return self.client.chat.completions.create(
            model=self.model_config.model_name,
            messages=messages,
            **api_kwargs
        )
    
    def _chat_completion_stream(self, messages: List[Dict[str, str]]) -> ChatCompletion:
        """
        Make a streaming call to the OpenAI Chat Completions API and return the final response.
        """
        logger.debug(f"Starting streaming chat completion for model: {self.model_config.model_name}")
        
        # Prepare kwargs for streaming, removing 'stream' to avoid duplication and filtering internal params
        stream_kwargs = self._filter_api_kwargs({k: v for k, v in self.model_config.kwargs.items() if k != 'stream'})
        
        try:
            # Create the stream with usage tracking
            stream = self.client.chat.completions.create(
                model=self.model_config.model_name,
                messages=messages,
                stream=True,
                stream_options={"include_usage": True},
                **stream_kwargs
            )
            
            # Process the stream and collect data
            content_chunks = []
            last_chunk = None
            finish_reason = "stop"
            chunk_count = 0
            
            for chunk in stream:
                last_chunk = chunk
                chunk_count += 1

                # Extract content from chunk
                delta_content = ""
                if chunk.choices:
                    delta_content = chunk.choices[0].delta.content or ""
                    if delta_content:
                        content_chunks.append(delta_content)

                # Track finish reason
                if chunk.choices and chunk.choices[0].finish_reason:
                    finish_reason = chunk.choices[0].finish_reason

                if chunk_count % 200 == 0 and logger.isEnabledFor(logging.DEBUG):
                    logger.debug(
                        f"Streaming progress: {chunk_count} chunks received; "
                        f"latest chunk chars={len(delta_content)}"
                    )

            # Build final response
            final_content = ''.join(content_chunks)
            
            # Get usage data and metadata from last chunk
            usage_data = last_chunk.usage if last_chunk and hasattr(last_chunk, 'usage') else None
            response_id = last_chunk.id if last_chunk else f"stream-{int(time.time())}"
            
            if not usage_data:
                logger.warning("No usage data received from streaming response")
                usage_data = CompletionUsage(
                    prompt_tokens=0,
                    completion_tokens=0,
                    total_tokens=0
                )
            
            logger.debug(
                f"Streaming complete. Chunks received: {chunk_count}, Content length: {len(final_content)}"
            )
            
            return ChatCompletion(
                id=response_id,
                choices=[
                    OpenAIChoice(
                        finish_reason=finish_reason,
                        index=0,
                        message=ChatCompletionMessage(
                            content=final_content,
                            role='assistant'
                        ),
                        logprobs=None
                    )
                ],
                created=int(time.time()),
                model=self.model_config.model_name,
                object='chat.completion',
                usage=usage_data
            )
            
        except Exception as e:
            logger.error(f"Error during streaming: {e}")
            raise
    
    def _delete_response(self, response_id: str) -> None:
        """
        Delete a response from the OpenAI API
        """
        self.client.responses.delete(response_id)
    
    def _responses(self, messages: List[Dict[str, str]]) -> Any:
        """
        Make a call to the OpenAI Responses API
        """
        api_kwargs = self._filter_api_kwargs(self.model_config.kwargs)
        resp = self.client.responses.create(
            model=self.model_config.model_name,
            input=messages,
            **api_kwargs
        )

        # For background mode
        if "background" in self.model_config.kwargs and self.model_config.kwargs["background"]:
            while resp.status in {"queued", "in_progress"}:
                sleep(10)
                resp = self.client.responses.retrieve(resp.id)

            # Delete the background response after we're done with it
            try:
                self._delete_response(resp.id)
            except Exception as e:
                logger.warning(f"Error deleting response: {e}")

        return resp
    
    

    def _responses_stream(self, messages: List[Dict[str, str]]) -> Any:
        """
        Make a streaming call to the OpenAI Responses API and return the final response.
        """
        logger.debug(f"Starting streaming responses for model: {self.model_config.model_name}")
        
        # Prepare kwargs for streaming, removing 'stream' to avoid duplication and filtering internal params
        stream_kwargs = self._filter_api_kwargs({k: v for k, v in self.model_config.kwargs.items() if k != 'stream'})
        
        try:
            # Create the stream
            stream = self.client.responses.create(
                model=self.model_config.model_name,
                input=messages,
                stream=True,
                **stream_kwargs
            )
            
            # Process the stream and collect data
            content_chunks = []
            response_id = None
            finish_reason = "stop"
            usage_data = None
            chunk_count = 0
            
            for chunk in stream:
                chunk_count += 1

                # Extract response ID
                if chunk.type == 'response.created':
                    response_id = chunk.response.id
                
                # Extract content deltas
                if chunk.type == 'response.output_text.delta':
                    delta = chunk.delta or ""
                    if delta:
                        content_chunks.append(delta)
                    delta_length = len(delta)
                else:
                    delta_length = 0
                
                # Track finish reason
                if hasattr(chunk, 'finish_reason') and chunk.finish_reason:
                    finish_reason = chunk.finish_reason
                
                # Extract usage data
                if hasattr(chunk, 'response') and chunk.response:
                    usage_data = self._get_usage(chunk.response)

                if chunk_count % 10 == 0 and logger.isEnabledFor(logging.DEBUG):
                    logger.debug(
                        f"Streaming progress: {chunk_count} chunks received; "
                        f"last chunk type={getattr(chunk, 'type', 'unknown')}, chars={delta_length}"
                    )
            
            # Build final response
            final_content = ''.join(content_chunks)
            response_id = response_id or f"stream-{int(time.time())}"
            
            logger.debug(
                f"Streaming complete. Chunks received: {chunk_count}, Content length: {len(final_content)}"
            )
            
            return _ResponsesResponse(
                model_name=self.model_config.model_name,
                content=final_content,
                usage_data=usage_data,
                response_id=response_id,
                finish_reason=finish_reason
            )
            
        except Exception as e:
            logger.error(f"Error during streaming: {e}")
            raise

    @abc.abstractmethod
    def extract_json_from_response(self, input_response: str) -> list[list[int]] | None:
        """
        Extract JSON from the provider's response.
        Subclasses must implement this based on expected response format.
        """
        pass
        
    def _get_usage(self, response: Any) -> Usage:
        """
        Get the usage from the response and convert to our Usage schema.
        Handles OpenAI ChatCompletion, Responses API, and already-converted Usage objects.
        """
        if not hasattr(response, 'usage') or not response.usage:
            return Usage(
                prompt_tokens=0,
                completion_tokens=0, 
                total_tokens=0,
                completion_tokens_details=CompletionTokensDetails(
                    reasoning_tokens=0,
                    accepted_prediction_tokens=0,
                    rejected_prediction_tokens=0
                )
            )
            
        raw_usage = response.usage
        
        # If it's already our custom Usage object, return it directly
        if isinstance(raw_usage, Usage):
            return raw_usage
        
        # Handle different API response types
        if self.model_config.api_type == APIType.CHAT_COMPLETIONS:
            # OpenAI Chat Completions API - uses CompletionUsage with prompt_tokens/completion_tokens
            prompt_tokens = getattr(raw_usage, 'prompt_tokens', 0)
            completion_tokens = getattr(raw_usage, 'completion_tokens', 0)
            total_tokens = getattr(raw_usage, 'total_tokens', prompt_tokens + completion_tokens)
            
            # Extract reasoning tokens if available (for reasoning models like o1)
            reasoning_tokens = 0
            if hasattr(raw_usage, 'completion_tokens_details') and raw_usage.completion_tokens_details:
                reasoning_tokens = getattr(raw_usage.completion_tokens_details, 'reasoning_tokens', 0)
                
        elif self.model_config.api_type == APIType.RESPONSES:
            # OpenAI Responses API - uses input_tokens/output_tokens
            prompt_tokens = getattr(raw_usage, 'input_tokens', 0)
            completion_tokens = getattr(raw_usage, 'output_tokens', 0)
            total_tokens = getattr(raw_usage, 'total_tokens', prompt_tokens + completion_tokens)
            
            # Extract reasoning tokens for responses API
            reasoning_tokens = 0
            if hasattr(raw_usage, 'output_tokens_details') and raw_usage.output_tokens_details:
                reasoning_tokens = getattr(raw_usage.output_tokens_details, 'reasoning_tokens', 0)
                
        
        # If no explicit reasoning tokens but total > prompt + completion, infer reasoning
        if reasoning_tokens == 0 and total_tokens > (prompt_tokens + completion_tokens):
            reasoning_tokens = total_tokens - (prompt_tokens + completion_tokens)
        
        # Create our standardized Usage object
        return Usage(
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            total_tokens=total_tokens,
            completion_tokens_details=CompletionTokensDetails(
                reasoning_tokens=reasoning_tokens,
                accepted_prediction_tokens=completion_tokens,
                rejected_prediction_tokens=0
            )
        )

    def _get_reasoning_summary(self, response: Any) -> Optional[List[Dict[str, Any]]]:
        """
        Extract reasoning summary from the response if available (primarily for Responses API).
        """
        reasoning_summary = None
        if self.model_config.api_type == APIType.RESPONSES:
            # Safely access potential reasoning summary
            if hasattr(response, 'reasoning') and response.reasoning and hasattr(response.reasoning, 'summary'):
                reasoning_summary = response.reasoning.summary # Will be None if not present
        # Chat Completions API does not currently provide a separate summary field
        return reasoning_summary

    def _get_content(self, response: Any) -> str:
        """
        Get the content from the response
        """
        api_type = self.model_config.api_type
        if api_type == APIType.CHAT_COMPLETIONS:
            content = response.choices[0].message.content or ""
        else:  # APIType.RESPONSES
            content = getattr(response, "output_text", "")
            if not content and getattr(response, "output", None):
                # Fallback to first text block
                content = response.output[0].content[0].text or ""
        return content.strip()


    def _get_role(self, response: Any) -> str:
        """Extract role from a standard OpenAI-like response object."""
        # Implementation copied from OpenAIAdapter
        role = "assistant" # Default role
        if self.model_config.api_type == APIType.CHAT_COMPLETIONS:
            if hasattr(response, 'choices') and response.choices and hasattr(response.choices[0], 'message'):
                 role = getattr(response.choices[0].message, 'role', "assistant") or "assistant"
        # Responses API implies assistant role for the main output
        return role
        
    def _normalize_to_responses_kwargs(self):
        """
        Normalize kwargs based on API type to handle different parameter names between chat completions and responses APIs
        """
        if self.model_config.api_type == APIType.RESPONSES:
            # Convert max_tokens and max_completion_tokens to max_output_tokens for responses API
            if "max_tokens" in self.model_config.kwargs:
                self.model_config.kwargs["max_output_tokens"] = self.model_config.kwargs.pop("max_tokens")
            if "max_completion_tokens" in self.model_config.kwargs:
                self.model_config.kwargs["max_output_tokens"] = self.model_config.kwargs.pop("max_completion_tokens") 

    def _calculate_cost(self, response: Any) -> Cost:
        """Calculate usage costs, validate token counts, and return a Cost object."""
        usage = self._get_usage(response)
        
        # Raw token counts from provider response (via _get_usage)
        pt_raw = usage.prompt_tokens
        ct_raw = usage.completion_tokens
        tt_raw = usage.total_tokens or 0
        rt_explicit = usage.completion_tokens_details.reasoning_tokens if usage.completion_tokens_details else 0

        # For OpenAI, we assume the raw tokens are correct and can be used directly for billing.
        # Determine effective token counts for cost calculation based on the two cases
        prompt_tokens_for_cost = pt_raw
        completion_tokens_for_cost = 0
        reasoning_tokens_for_cost = 0

        # Case A: Completion includes Reasoning (pt + ct == tt)
        # Here, ct_raw contains both reasoning and actual completion.
        if tt_raw == 0 or (pt_raw + ct_raw == tt_raw): 
            reasoning_tokens_for_cost = rt_explicit # Use explicit reasoning count if provided
            # Subtract explicit reasoning from raw completion to get actual completion
            completion_tokens_for_cost = max(0, ct_raw - reasoning_tokens_for_cost) 
            # Safety check: ensure computed total matches raw total if tt_raw was provided
            computed_total = pt_raw + ct_raw # In this case, ct_raw represents the full assistant output
        
        # Case B: Reasoning is Separate or Inferred (pt + ct < tt)
        # Here, ct_raw likely represents only the final answer tokens.
        else: 
            # Use explicit reasoning if provided, otherwise infer it
            reasoning_tokens_for_cost = rt_explicit if rt_explicit else tt_raw - (pt_raw + ct_raw)
            completion_tokens_for_cost = ct_raw # Raw completion is assumed to be separate
            # Calculate computed total based on the parts
            computed_total = pt_raw + completion_tokens_for_cost + reasoning_tokens_for_cost

        # Final Sanity Check: Compare computed total against provider's total (if provider gave one)
        if tt_raw and computed_total != tt_raw:
            from arcagi3.errors import TokenMismatchError  # Local import
            raise TokenMismatchError(
                f"Token count mismatch: API reports total {tt_raw}, "
                f"but computed P:{prompt_tokens_for_cost} + C:{completion_tokens_for_cost} + R:{reasoning_tokens_for_cost} = {computed_total}"
            )

        # Determine costs per token
        input_cost_per_token = self.model_config.pricing.input / 1_000_000
        output_cost_per_token = self.model_config.pricing.output / 1_000_000
        
        # Calculate costs based on the derived token counts
        prompt_cost = prompt_tokens_for_cost * input_cost_per_token
        # Cost for the 'actual' completion tokens (excluding reasoning in Case A)
        completion_cost = completion_tokens_for_cost * output_cost_per_token
        # Cost for the reasoning tokens
        reasoning_cost = reasoning_tokens_for_cost * output_cost_per_token
        # Total cost is the sum of all components
        total_cost = prompt_cost + completion_cost + reasoning_cost

        return Cost(
            prompt_cost=prompt_cost,
            completion_cost=completion_cost, # Cost of 'actual' completion
            reasoning_cost=reasoning_cost,   # Cost of reasoning part
            total_cost=total_cost,           # True total expenditure
        ) 

    def _consume_openai_stream(self, stream) -> StreamResponse:
        """Consume an OpenAI stream and extract content + usage"""
        full_content = []
        prompt_tokens = 0
        completion_tokens = 0
        reasoning_tokens = 0
        
        try:
            for chunk in stream:
                if hasattr(chunk, 'usage') and chunk.usage:
                    prompt_tokens = chunk.usage.prompt_tokens
                    completion_tokens = chunk.usage.completion_tokens
                    # Extract reasoning tokens if available
                    if hasattr(chunk.usage, 'completion_tokens_details') and chunk.usage.completion_tokens_details:
                        reasoning_tokens = getattr(chunk.usage.completion_tokens_details, 'reasoning_tokens', 0) or 0
                if hasattr(chunk, 'choices') and chunk.choices:
                    delta = chunk.choices[0].delta
                    if hasattr(delta, 'content') and delta.content:
                        full_content.append(delta.content)
        except Exception as e:
            logger.error(f"Error consuming stream: {e}")
        
        # Create a StreamResponse with reasoning_tokens attribute
        stream_response = StreamResponse(''.join(full_content), prompt_tokens, completion_tokens)
        stream_response.reasoning_tokens = reasoning_tokens
        return stream_response

    def extract_usage(self, response):
        """Extract token usage from response. Returns (prompt_tokens, completion_tokens, reasoning_tokens)"""
        # Handle consumed streams
        if isinstance(response, StreamResponse):
            # For streams, reasoning tokens would be in the response if available
            reasoning_tokens = getattr(response, 'reasoning_tokens', 0) if hasattr(response, 'reasoning_tokens') else 0
            return response.prompt_tokens, response.completion_tokens, reasoning_tokens
        
        # Check if it's an unconsumed stream - consume it!
        if 'Stream' in str(type(response)):
            # Consume the stream to get usage
            consumed = self._consume_openai_stream(response)
            # Cache the consumed response for extract_content to use
            self._last_consumed_stream = consumed
            reasoning_tokens = getattr(consumed, 'reasoning_tokens', 0) if hasattr(consumed, 'reasoning_tokens') else 0
            return consumed.prompt_tokens, consumed.completion_tokens, reasoning_tokens
        
        # Use _get_usage which handles both Chat Completions and Responses API formats
        usage = self._get_usage(response)
        reasoning_tokens = usage.completion_tokens_details.reasoning_tokens if usage.completion_tokens_details else 0
        return usage.prompt_tokens, usage.completion_tokens, reasoning_tokens
    
    def extract_content(self, response):
         # Check if it's an OpenAI stream - consume it first
        if hasattr(response, "output"):
            texts = []
            for item in response.output:
                # Skip thinking blocks (reasoning)
                if getattr(item, "type", None) == "reasoning":
                    continue

                # Assistant message
                if getattr(item, "type", None) == "message":
                    if hasattr(item, "content"):
                        for part in item.content:
                            if getattr(part, "type", None) == "output_text":
                                texts.append(part.text or "")
            return "\n".join(texts).strip()

        # Check if we have a cached consumed stream from extract_usage()
        if hasattr(self, '_last_consumed_stream') and self._last_consumed_stream:
            content = self._last_consumed_stream.content
            self._last_consumed_stream = None  # Clear cache
            return content

        if 'Stream' in str(type(response)):
            # Consume the stream and get the final response
            consumed = self._consume_openai_stream(response)
            return consumed.content
        
        # OpenAI format - keep it simple like original
        if hasattr(response, 'choices') and response.choices:
            content = response.choices[0].message.content
            if content is None:
                logger.warning("OpenAI returned None content")
                return ""
            return content
        
        logger.warning(f"Unknown response format. Type: {type(response)}")
        return ""
    
    @retry_with_exponential_backoff(max_retries=3)
    def call_provider(self, messages):
        # For other providers, try OpenAI-compatible format
        api_kwargs = self._filter_api_kwargs(self.model_config.kwargs)
        try:
            return self.client.chat.completions.create(
                model=self.model_config.model_name,
                messages=messages,
                **api_kwargs
            )
        except AttributeError:
            # Fallback to direct client call
            return self.client.create(
                model=self.model_config.model_name,
                messages=messages,
                **api_kwargs
            )